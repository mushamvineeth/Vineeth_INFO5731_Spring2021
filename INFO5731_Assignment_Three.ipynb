{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Three.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mushamvineeth/Vineeth_INFO5731_Spring2021/blob/main/INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2615c30-09d7-4979-d345-c688c7922d38"
      },
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n",
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       |   Unzipping corpora/omw.zip.\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet.zip.\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | \n",
            "     Done downloading collection all\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwRZ3xy-hGQu",
        "outputId": "1e120118-9d4f-4acb-d343-a0e91245d986"
      },
      "source": [
        "!apt-get update\r\n",
        "!apt install chromium-chromedriver\r\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\r\n",
        "!pip install selenium"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [1 InRelease 14.2 kB/88.7\r0% [Waiting for headers] [Connected to cloud.r-project.org (65.8.180.43)] [Wait\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connected to cloud.r-proje\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connected to cloud.r-proje\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rGet:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [4 In\r                                                                               \rIgn:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [4 In\r                                                                               \rGet:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [4 InRelease 14.2 kB/15.9 k\r                                                                               \rGet:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [4 InRelease 14.2 kB/15.9 k\r                                                                               \rHit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [4 InRelease 14.2 kB/15.9 k\r                                                                               \rGet:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [333 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,398 kB]\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,995 kB]\n",
            "Hit:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:18 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [49.4 kB]\n",
            "Ign:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [603 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,746 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [363 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,425 kB]\n",
            "Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [894 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,165 kB]\n",
            "Fetched 12.2 MB in 4s (3,305 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 83.2 MB of archives.\n",
            "After this operation, 282 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 89.0.4389.82-0ubuntu0.18.04.1 [1,127 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 89.0.4389.82-0ubuntu0.18.04.1 [73.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 89.0.4389.82-0ubuntu0.18.04.1 [3,800 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 89.0.4389.82-0ubuntu0.18.04.1 [4,697 kB]\n",
            "Fetched 83.2 MB in 3s (24.6 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_89.0.4389.82-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (89.0.4389.82-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_89.0.4389.82-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (89.0.4389.82-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_89.0.4389.82-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (89.0.4389.82-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_89.0.4389.82-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (89.0.4389.82-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (89.0.4389.82-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (89.0.4389.82-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (89.0.4389.82-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (89.0.4389.82-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeryXgL6hVEw"
      },
      "source": [
        "import time\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from selenium import webdriver\r\n",
        "from selenium.webdriver.support.ui import WebDriverWait as wait\r\n",
        "from selenium.webdriver.common.by import By\r\n",
        "options = webdriver.ChromeOptions()\r\n",
        "options.add_argument('-headless')\r\n",
        "options.add_argument('-no-sandbox')\r\n",
        "options.add_argument('-disable-dev-shm-usage')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "kTO1epr6hYpM",
        "outputId": "db4339c7-4adc-4dc9-d83d-7d9382a4d4fc"
      },
      "source": [
        "driver = webdriver.Chrome('chromedriver',options=options)\r\n",
        "Review_Link = 'https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv'\r\n",
        "Titles= []\r\n",
        "Reviews= []\r\n",
        "driver.get(Review_Link)\r\n",
        "for p in range(4):\r\n",
        "  driver.find_element_by_class_name(\"ipl-load-more__button\").click()\r\n",
        "  time.sleep(5)\r\n",
        "  Complete_Titles = driver.find_elements(By.CLASS_NAME, \"title\")\r\n",
        "  Complete_Reviews = driver.find_elements(By.CLASS_NAME, \"text\")\r\n",
        "for m, n in zip(Complete_Reviews, Complete_Reviews):\r\n",
        "      Titles.append((m.text).replace('\\n',''))\r\n",
        "      Reviews.append(n.text)\r\n",
        "Data_Frame = pd.DataFrame(list(zip(Titles, Reviews)), columns =['Titles', 'Reviews'])\r\n",
        "Data_Frame"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Titles</th>\n",
              "      <th>Reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
              "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Joaquin Phoenix gives a tour de force performa...</td>\n",
              "      <td>Joaquin Phoenix gives a tour de force performa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>I didnt really know much about this movie. I h...</td>\n",
              "      <td>I didnt really know much about this movie. I h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>Film was directed and produced poorly. Opportu...</td>\n",
              "      <td>Film was directed and produced poorly. Opportu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>Wow, what a movie! I have to admit, When I fir...</td>\n",
              "      <td>Wow, what a movie! I have to admit, When I fir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>This movie is poorly done as to how it tells t...</td>\n",
              "      <td>This movie is poorly done as to how it tells t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Titles                                            Reviews\n",
              "0    Every once in a while a movie comes, that trul...  Every once in a while a movie comes, that trul...\n",
              "1    This is a movie that only those who have felt ...  This is a movie that only those who have felt ...\n",
              "2    Truly a masterpiece, The Best Hollywood film o...  Truly a masterpiece, The Best Hollywood film o...\n",
              "3    Joaquin Phoenix gives a tour de force performa...  Joaquin Phoenix gives a tour de force performa...\n",
              "4    Most of the time movies are anticipated like t...  Most of the time movies are anticipated like t...\n",
              "..                                                 ...                                                ...\n",
              "120  I didnt really know much about this movie. I h...  I didnt really know much about this movie. I h...\n",
              "121                                                                                                      \n",
              "122  Film was directed and produced poorly. Opportu...  Film was directed and produced poorly. Opportu...\n",
              "123  Wow, what a movie! I have to admit, When I fir...  Wow, what a movie! I have to admit, When I fir...\n",
              "124  This movie is poorly done as to how it tells t...  This movie is poorly done as to how it tells t...\n",
              "\n",
              "[125 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "7TfCntieoS0l",
        "outputId": "fb6e53b7-8eea-4c8e-d467-d04d06f263e8"
      },
      "source": [
        "#Lower Case\r\n",
        "Data_Frame['After Converting to lower case'] = Data_Frame['Reviews'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\r\n",
        "Data_Frame\r\n",
        "\r\n",
        "#Punctuation Removal\r\n",
        "Data_Frame['After Removing Punctuation'] = Data_Frame['After Converting to lower case'].str.replace('[^\\w\\s]','')\r\n",
        "Data_Frame\r\n",
        "\r\n",
        "#Special Charachters Removal\r\n",
        "import re\r\n",
        "Data_Frame['After Removing Special Charachters'] = Data_Frame['After Removing Punctuation'].apply(lambda x: ''.join(re.sub(r\"[^a-zA-Z0-9]+\", ' ', charctr) for charctr in x ))\r\n",
        "Data_Frame\r\n",
        "\r\n",
        "#Stopwords Removal\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stop = stopwords.words('english')\r\n",
        "Data_Frame['After Removing Stopwords'] = Data_Frame['After Removing Punctuation'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\r\n",
        "Data_Frame\r\n",
        "\r\n",
        "#Spelling Correction\r\n",
        "from textblob import TextBlob\r\n",
        "Data_Frame['After Spelling Correction'] = Data_Frame['After Removing Stopwords'].apply(lambda x: str(TextBlob(x).correct()))\r\n",
        "Data_Frame\r\n",
        "\r\n",
        "#Tokenization\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "Data_Frame['After Tokenization'] = Data_Frame['After Spelling Correction'].apply(lambda x: TextBlob(x).words)\r\n",
        "Data_Frame\r\n",
        "\r\n",
        "\r\n",
        "#Stemming\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "ps = PorterStemmer()\r\n",
        "Data_Frame['After Stemming'] = Data_Frame['After Tokenization'].apply(lambda x: \" \".join([ps.stem(word) for word in x]))\r\n",
        "Data_Frame\r\n",
        "\r\n",
        "#Lemmatization\r\n",
        "from textblob import Word\r\n",
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n",
        "Data_Frame['After Lemmatization'] = Data_Frame['After Stemming'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "Data_Frame.to_csv('/content/sample_data/reviwes.csv',index=False)\r\n",
        "Data_Frame\r\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Titles</th>\n",
              "      <th>Reviews</th>\n",
              "      <th>After Converting to lower case</th>\n",
              "      <th>After Removing Punctuation</th>\n",
              "      <th>After Removing Special Charachters</th>\n",
              "      <th>After Removing Stopwords</th>\n",
              "      <th>After Spelling Correction</th>\n",
              "      <th>After Tokenization</th>\n",
              "      <th>After Stemming</th>\n",
              "      <th>After Lemmatization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "      <td>every once in a while a movie comes, that trul...</td>\n",
              "      <td>every once in a while a movie comes that truly...</td>\n",
              "      <td>every once in a while a movie comes that truly...</td>\n",
              "      <td>every movie comes truly makes impact joaquins ...</td>\n",
              "      <td>every movie comes truly makes impact joaquins ...</td>\n",
              "      <td>[every, movie, comes, truly, makes, impact, jo...</td>\n",
              "      <td>everi movi come truli make impact joaquin perf...</td>\n",
              "      <td>everi movi come truli make impact joaquin perf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "      <td>this is a movie that only those who have felt ...</td>\n",
              "      <td>this is a movie that only those who have felt ...</td>\n",
              "      <td>this is a movie that only those who have felt ...</td>\n",
              "      <td>movie felt alone isolated truly relate underst...</td>\n",
              "      <td>movie felt alone isolated truly relate underst...</td>\n",
              "      <td>[movie, felt, alone, isolated, truly, relate, ...</td>\n",
              "      <td>movi felt alon isol truli relat understand mot...</td>\n",
              "      <td>movi felt alon isol truli relat understand mot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
              "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
              "      <td>truly a masterpiece, the best hollywood film o...</td>\n",
              "      <td>truly a masterpiece the best hollywood film of...</td>\n",
              "      <td>truly a masterpiece the best hollywood film of...</td>\n",
              "      <td>truly masterpiece best hollywood film 2019 one...</td>\n",
              "      <td>truly masterpiece best hollywood film 2019 one...</td>\n",
              "      <td>[truly, masterpiece, best, hollywood, film, 20...</td>\n",
              "      <td>truli masterpiec best hollywood film 2019 one ...</td>\n",
              "      <td>truli masterpiec best hollywood film 2019 one ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Joaquin Phoenix gives a tour de force performa...</td>\n",
              "      <td>Joaquin Phoenix gives a tour de force performa...</td>\n",
              "      <td>joaquin phoenix gives a tour de force performa...</td>\n",
              "      <td>joaquin phoenix gives a tour de force performa...</td>\n",
              "      <td>joaquin phoenix gives a tour de force performa...</td>\n",
              "      <td>joaquin phoenix gives tour de force performanc...</td>\n",
              "      <td>joaquin phoenix gives tour de force performanc...</td>\n",
              "      <td>[joaquin, phoenix, gives, tour, de, force, per...</td>\n",
              "      <td>joaquin phoenix give tour de forc perform fear...</td>\n",
              "      <td>joaquin phoenix give tour de forc perform fear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "      <td>most of the time movies are anticipated like t...</td>\n",
              "      <td>most of the time movies are anticipated like t...</td>\n",
              "      <td>most of the time movies are anticipated like t...</td>\n",
              "      <td>time movies anticipated like end falling short...</td>\n",
              "      <td>time moves anticipated like end falling short ...</td>\n",
              "      <td>[time, moves, anticipated, like, end, falling,...</td>\n",
              "      <td>time move anticip like end fall short way shor...</td>\n",
              "      <td>time move anticip like end fall short way shor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>I didnt really know much about this movie. I h...</td>\n",
              "      <td>I didnt really know much about this movie. I h...</td>\n",
              "      <td>i didnt really know much about this movie. i h...</td>\n",
              "      <td>i didnt really know much about this movie i ha...</td>\n",
              "      <td>i didnt really know much about this movie i ha...</td>\n",
              "      <td>didnt really know much movie hadnt seen traile...</td>\n",
              "      <td>didn really know much movie hadn seen trailer ...</td>\n",
              "      <td>[didn, really, know, much, movie, hadn, seen, ...</td>\n",
              "      <td>didn realli know much movi hadn seen trailer a...</td>\n",
              "      <td>didn realli know much movi hadn seen trailer a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>Film was directed and produced poorly. Opportu...</td>\n",
              "      <td>Film was directed and produced poorly. Opportu...</td>\n",
              "      <td>film was directed and produced poorly. opportu...</td>\n",
              "      <td>film was directed and produced poorly opportun...</td>\n",
              "      <td>film was directed and produced poorly opportun...</td>\n",
              "      <td>film directed produced poorly opportunity take...</td>\n",
              "      <td>film directed produced poorly opportunity take...</td>\n",
              "      <td>[film, directed, produced, poorly, opportunity...</td>\n",
              "      <td>film direct produc poorli opportun take advant...</td>\n",
              "      <td>film direct produc poorli opportun take advant...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>Wow, what a movie! I have to admit, When I fir...</td>\n",
              "      <td>Wow, what a movie! I have to admit, When I fir...</td>\n",
              "      <td>wow, what a movie! i have to admit, when i fir...</td>\n",
              "      <td>wow what a movie i have to admit when i first ...</td>\n",
              "      <td>wow what a movie i have to admit when i first ...</td>\n",
              "      <td>wow movie admit first heard joaquin phoenix go...</td>\n",
              "      <td>now movie admit first heard joaquin phoenix go...</td>\n",
              "      <td>[now, movie, admit, first, heard, joaquin, pho...</td>\n",
              "      <td>now movi admit first heard joaquin phoenix go ...</td>\n",
              "      <td>now movi admit first heard joaquin phoenix go ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>This movie is poorly done as to how it tells t...</td>\n",
              "      <td>This movie is poorly done as to how it tells t...</td>\n",
              "      <td>this movie is poorly done as to how it tells t...</td>\n",
              "      <td>this movie is poorly done as to how it tells t...</td>\n",
              "      <td>this movie is poorly done as to how it tells t...</td>\n",
              "      <td>movie poorly done tells story dark movie showi...</td>\n",
              "      <td>movie poorly done tells story dark movie showi...</td>\n",
              "      <td>[movie, poorly, done, tells, story, dark, movi...</td>\n",
              "      <td>movi poorli done tell stori dark movi show pro...</td>\n",
              "      <td>movi poorli done tell stori dark movi show pro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Titles  ...                                After Lemmatization\n",
              "0    Every once in a while a movie comes, that trul...  ...  everi movi come truli make impact joaquin perf...\n",
              "1    This is a movie that only those who have felt ...  ...  movi felt alon isol truli relat understand mot...\n",
              "2    Truly a masterpiece, The Best Hollywood film o...  ...  truli masterpiec best hollywood film 2019 one ...\n",
              "3    Joaquin Phoenix gives a tour de force performa...  ...  joaquin phoenix give tour de forc perform fear...\n",
              "4    Most of the time movies are anticipated like t...  ...  time move anticip like end fall short way shor...\n",
              "..                                                 ...  ...                                                ...\n",
              "120  I didnt really know much about this movie. I h...  ...  didn realli know much movi hadn seen trailer a...\n",
              "121                                                     ...                                                   \n",
              "122  Film was directed and produced poorly. Opportu...  ...  film direct produc poorli opportun take advant...\n",
              "123  Wow, what a movie! I have to admit, When I fir...  ...  now movi admit first heard joaquin phoenix go ...\n",
              "124  This movie is poorly done as to how it tells t...  ...  movi poorli done tell stori dark movi show pro...\n",
              "\n",
              "[125 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxXDQvhKxi_t",
        "outputId": "65cfe7f5-f295-4f13-fbed-15b34f584603"
      },
      "source": [
        "import itertools\r\n",
        "import collections\r\n",
        "from nltk import ngrams\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "array= []\r\n",
        "for j in Data_Frame['After Lemmatization']:\r\n",
        "  array.append(word_tokenize(j))\r\n",
        "cleaned_data = [x for x in array if x != []]\r\n",
        "Repeat = list(itertools.chain.from_iterable(cleaned_data))\r\n",
        "\r\n",
        "trigrams = nltk.trigrams(Repeat)\r\n",
        "FD = nltk.FreqDist(trigrams)\r\n",
        "FD"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({('everi', 'movi', 'come'): 1,\n",
              "          ('movi', 'come', 'truli'): 1,\n",
              "          ('come', 'truli', 'make'): 1,\n",
              "          ('truli', 'make', 'impact'): 1,\n",
              "          ('make', 'impact', 'joaquin'): 1,\n",
              "          ('impact', 'joaquin', 'perform'): 1,\n",
              "          ('joaquin', 'perform', 'scenographi'): 1,\n",
              "          ('perform', 'scenographi', 'brillianc'): 1,\n",
              "          ('scenographi', 'brillianc', 'grotesqu'): 1,\n",
              "          ('brillianc', 'grotesqu', 'hunt'): 1,\n",
              "          ('grotesqu', 'hunt', 'cring'): 1,\n",
              "          ('hunt', 'cring', 'hard'): 1,\n",
              "          ('cring', 'hard', 'watch'): 1,\n",
              "          ('hard', 'watch', 'time'): 1,\n",
              "          ('watch', 'time', 'mesmer'): 1,\n",
              "          ('time', 'mesmer', 'wont'): 1,\n",
              "          ('mesmer', 'wont', 'blink'): 1,\n",
              "          ('wont', 'blink', 'eye'): 1,\n",
              "          ('blink', 'eye', 'watch'): 1,\n",
              "          ('eye', 'watch', 'tragic'): 1,\n",
              "          ('watch', 'tragic', 'serious'): 1,\n",
              "          ('tragic', 'serious', 'funni'): 1,\n",
              "          ('serious', 'funni', 'moment'): 1,\n",
              "          ('funni', 'moment', 'emot'): 1,\n",
              "          ('moment', 'emot', 'rollercoast'): 1,\n",
              "          ('emot', 'rollercoast', 'sometim'): 1,\n",
              "          ('rollercoast', 'sometim', 'multipl'): 1,\n",
              "          ('sometim', 'multipl', 'emot'): 1,\n",
              "          ('multipl', 'emot', 'pop'): 1,\n",
              "          ('emot', 'pop', 'time'): 1,\n",
              "          ('pop', 'time', 'far'): 1,\n",
              "          ('time', 'far', 'typic'): 1,\n",
              "          ('far', 'typic', 'actionriddl'): 1,\n",
              "          ('typic', 'actionriddl', 'predict'): 1,\n",
              "          ('actionriddl', 'predict', 'superhero'): 1,\n",
              "          ('predict', 'superhero', 'movi'): 1,\n",
              "          ('superhero', 'movi', 'proper'): 1,\n",
              "          ('movi', 'proper', 'psycholog'): 1,\n",
              "          ('proper', 'psycholog', 'thrillerdrama'): 1,\n",
              "          ('psycholog', 'thrillerdrama', 'singl'): 1,\n",
              "          ('thrillerdrama', 'singl', 'best'): 1,\n",
              "          ('singl', 'best', 'charact'): 1,\n",
              "          ('best', 'charact', 'develop'): 1,\n",
              "          ('charact', 'develop', 'ever'): 1,\n",
              "          ('develop', 'ever', 'seen'): 1,\n",
              "          ('ever', 'seen', 'movi'): 2,\n",
              "          ('seen', 'movi', 'felt'): 1,\n",
              "          ('movi', 'felt', 'alon'): 1,\n",
              "          ('felt', 'alon', 'isol'): 1,\n",
              "          ('alon', 'isol', 'truli'): 1,\n",
              "          ('isol', 'truli', 'relat'): 1,\n",
              "          ('truli', 'relat', 'understand'): 1,\n",
              "          ('relat', 'understand', 'motiv'): 1,\n",
              "          ('understand', 'motiv', 'feel'): 1,\n",
              "          ('motiv', 'feel', 'sorri'): 1,\n",
              "          ('feel', 'sorri', 'charact'): 1,\n",
              "          ('sorri', 'charact', 'lot'): 1,\n",
              "          ('charact', 'lot', 'peopl'): 1,\n",
              "          ('lot', 'peopl', 'see'): 1,\n",
              "          ('peopl', 'see', 'movi'): 1,\n",
              "          ('see', 'movi', 'think'): 1,\n",
              "          ('movi', 'think', 'encourag'): 1,\n",
              "          ('think', 'encourag', 'violenc'): 1,\n",
              "          ('encourag', 'violenc', 'truli'): 1,\n",
              "          ('violenc', 'truli', 'movi'): 1,\n",
              "          ('truli', 'movi', 'encourag'): 1,\n",
              "          ('movi', 'encourag', 'everi'): 1,\n",
              "          ('encourag', 'everi', 'one'): 1,\n",
              "          ('everi', 'one', 'u'): 1,\n",
              "          ('one', 'u', 'becom'): 1,\n",
              "          ('u', 'becom', 'better'): 1,\n",
              "          ('becom', 'better', 'person'): 1,\n",
              "          ('better', 'person', 'treat'): 1,\n",
              "          ('person', 'treat', 'everyon'): 1,\n",
              "          ('treat', 'everyon', 'respect'): 1,\n",
              "          ('everyon', 'respect', 'make'): 1,\n",
              "          ('respect', 'make', 'feel'): 1,\n",
              "          ('make', 'feel', 'like'): 1,\n",
              "          ('feel', 'like', 'belong'): 1,\n",
              "          ('like', 'belong', 'world'): 1,\n",
              "          ('belong', 'world', 'instead'): 1,\n",
              "          ('world', 'instead', 'make'): 1,\n",
              "          ('instead', 'make', 'feel'): 1,\n",
              "          ('make', 'feel', 'isol'): 1,\n",
              "          ('feel', 'isol', 'truli'): 1,\n",
              "          ('isol', 'truli', 'masterpiec'): 1,\n",
              "          ('truli', 'masterpiec', 'best'): 1,\n",
              "          ('masterpiec', 'best', 'hollywood'): 1,\n",
              "          ('best', 'hollywood', 'film'): 1,\n",
              "          ('hollywood', 'film', '2019'): 1,\n",
              "          ('film', '2019', 'one'): 1,\n",
              "          ('2019', 'one', 'best'): 1,\n",
              "          ('one', 'best', 'film'): 2,\n",
              "          ('best', 'film', 'decad'): 2,\n",
              "          ('film', 'decad', 'truli'): 1,\n",
              "          ('decad', 'truli', 'best'): 1,\n",
              "          ('truli', 'best', 'film'): 1,\n",
              "          ('best', 'film', 'bring'): 1,\n",
              "          ('film', 'bring', 'comic'): 1,\n",
              "          ('bring', 'comic', 'book'): 1,\n",
              "          ('comic', 'book', 'willingli'): 1,\n",
              "          ('book', 'willingli', 'realist'): 1,\n",
              "          ('willingli', 'realist', 'real'): 1,\n",
              "          ('realist', 'real', 'if'): 1,\n",
              "          ('real', 'if', 'remark'): 1,\n",
              "          ('if', 'remark', 'direct'): 1,\n",
              "          ('remark', 'direct', 'cinematographi'): 1,\n",
              "          ('direct', 'cinematographi', 'music'): 1,\n",
              "          ('cinematographi', 'music', 'act'): 1,\n",
              "          ('music', 'act', 'peopl'): 1,\n",
              "          ('act', 'peopl', 'surpris'): 1,\n",
              "          ('peopl', 'surpris', 'find'): 1,\n",
              "          ('surpris', 'find', 'disturb'): 1,\n",
              "          ('find', 'disturb', 'violent'): 1,\n",
              "          ('disturb', 'violent', 'necess'): 1,\n",
              "          ('violent', 'necess', 'messag'): 1,\n",
              "          ('necess', 'messag', 'societi'): 1,\n",
              "          ('messag', 'societi', 'reflect'): 1,\n",
              "          ('societi', 'reflect', 'underappreciatedunrecognizedbulli'): 1,\n",
              "          ('reflect', 'underappreciatedunrecognizedbulli', 'peopl'): 1,\n",
              "          ('underappreciatedunrecognizedbulli', 'peopl', 'prove'): 1,\n",
              "          ('peopl', 'prove', 'someth'): 1,\n",
              "          ('prove', 'someth', 'way'): 1,\n",
              "          ('someth', 'way', 'show'): 1,\n",
              "          ('way', 'show', 'class'): 1,\n",
              "          ('show', 'class', 'differ'): 1,\n",
              "          ('class', 'differ', 'corrupt'): 1,\n",
              "          ('differ', 'corrupt', 'rich'): 1,\n",
              "          ('corrupt', 'rich', 'talent'): 1,\n",
              "          ('rich', 'talent', 'rule'): 1,\n",
              "          ('talent', 'rule', 'other'): 1,\n",
              "          ('rule', 'other', 'around'): 1,\n",
              "          ('other', 'around', 'exagger'): 1,\n",
              "          ('around', 'exagger', 'that'): 1,\n",
              "          ('exagger', 'that', 'make'): 1,\n",
              "          ('that', 'make', 'differ'): 1,\n",
              "          ('make', 'differ', 'believ'): 1,\n",
              "          ('differ', 'believ', 'could'): 1,\n",
              "          ('believ', 'could', 'multipl'): 1,\n",
              "          ('could', 'multipl', 'joke'): 1,\n",
              "          ('multipl', 'joke', 'live'): 1,\n",
              "          ('joke', 'live', 'societi'): 1,\n",
              "          ('live', 'societi', 'could'): 1,\n",
              "          ('societi', 'could', 'shake'): 1,\n",
              "          ('could', 'shake', 'around'): 1,\n",
              "          ('shake', 'around', 'much'): 1,\n",
              "          ('around', 'much', 'bitter'): 1,\n",
              "          ('much', 'bitter', 'way'): 1,\n",
              "          ('bitter', 'way', 'film'): 1,\n",
              "          ('way', 'film', 'show'): 1,\n",
              "          ('film', 'show', 'make'): 1,\n",
              "          ('show', 'make', 'peopl'): 1,\n",
              "          ('make', 'peopl', 'comfort'): 1,\n",
              "          ('peopl', 'comfort', 'peopl'): 1,\n",
              "          ('comfort', 'peopl', 'consid'): 1,\n",
              "          ('peopl', 'consid', 'wake'): 1,\n",
              "          ('consid', 'wake', 'call'): 1,\n",
              "          ('wake', 'call', 'messag'): 1,\n",
              "          ('call', 'messag', 'first'): 1,\n",
              "          ('messag', 'first', 'film'): 1,\n",
              "          ('first', 'film', 'perfect'): 1,\n",
              "          ('film', 'perfect', 'film'): 1,\n",
              "          ('perfect', 'film', 'joaquin'): 1,\n",
              "          ('film', 'joaquin', 'phoenix'): 1,\n",
              "          ('joaquin', 'phoenix', 'give'): 2,\n",
              "          ('phoenix', 'give', 'tour'): 1,\n",
              "          ('give', 'tour', 'de'): 1,\n",
              "          ('tour', 'de', 'forc'): 1,\n",
              "          ('de', 'forc', 'perform'): 1,\n",
              "          ('forc', 'perform', 'fearless'): 1,\n",
              "          ('perform', 'fearless', 'stun'): 1,\n",
              "          ('fearless', 'stun', 'emot'): 1,\n",
              "          ('stun', 'emot', 'depth'): 1,\n",
              "          ('emot', 'depth', 'physic'): 1,\n",
              "          ('depth', 'physic', 'imposs'): 1,\n",
              "          ('physic', 'imposs', 'talk'): 1,\n",
              "          ('imposs', 'talk', 'without'): 1,\n",
              "          ('talk', 'without', 'referenc'): 1,\n",
              "          ('without', 'referenc', 'heath'): 1,\n",
              "          ('referenc', 'heath', 'ledger'): 1,\n",
              "          ('heath', 'ledger', 'oscarwin'): 1,\n",
              "          ('ledger', 'oscarwin', 'perform'): 1,\n",
              "          ('oscarwin', 'perform', 'dark'): 1,\n",
              "          ('perform', 'dark', 'knight'): 1,\n",
              "          ('dark', 'knight', 'wide'): 1,\n",
              "          ('knight', 'wide', 'consid'): 1,\n",
              "          ('wide', 'consid', 'definit'): 1,\n",
              "          ('consid', 'definit', 'liveact'): 1,\n",
              "          ('definit', 'liveact', 'portray'): 1,\n",
              "          ('liveact', 'portray', 'joke'): 1,\n",
              "          ('portray', 'joke', 'let'): 1,\n",
              "          ('joke', 'let', 'talk'): 1,\n",
              "          ('let', 'talk', 'fact'): 1,\n",
              "          ('talk', 'fact', 'everyon'): 1,\n",
              "          ('fact', 'everyon', 'go'): 1,\n",
              "          ('everyon', 'go', 'stun'): 1,\n",
              "          ('go', 'stun', 'phoenix'): 1,\n",
              "          ('stun', 'phoenix', 'accomplish'): 1,\n",
              "          ('phoenix', 'accomplish', 'mani'): 1,\n",
              "          ('accomplish', 'mani', 'thought'): 1,\n",
              "          ('mani', 'thought', 'imposs'): 1,\n",
              "          ('thought', 'imposs', 'portray'): 1,\n",
              "          ('imposs', 'portray', 'match'): 1,\n",
              "          ('portray', 'match', 'potenti'): 1,\n",
              "          ('match', 'potenti', 'exce'): 1,\n",
              "          ('potenti', 'exce', 'dark'): 1,\n",
              "          ('exce', 'dark', 'knight'): 1,\n",
              "          ('dark', 'knight', 'clown'): 1,\n",
              "          ('knight', 'clown', 'princ'): 1,\n",
              "          ('clown', 'princ', 'crime'): 3,\n",
              "          ('princ', 'crime', 'time'): 1,\n",
              "          ('crime', 'time', 'move'): 1,\n",
              "          ('time', 'move', 'anticip'): 1,\n",
              "          ('move', 'anticip', 'like'): 1,\n",
              "          ('anticip', 'like', 'end'): 1,\n",
              "          ('like', 'end', 'fall'): 1,\n",
              "          ('end', 'fall', 'short'): 1,\n",
              "          ('fall', 'short', 'way'): 1,\n",
              "          ('short', 'way', 'short'): 1,\n",
              "          ('way', 'short', 'joke'): 1,\n",
              "          ('short', 'joke', 'first'): 1,\n",
              "          ('joke', 'first', 'time'): 2,\n",
              "          ('first', 'time', 'happi'): 1,\n",
              "          ('time', 'happi', 'hope'): 1,\n",
              "          ('happi', 'hope', 'plea'): 1,\n",
              "          ('hope', 'plea', 'ignor'): 1,\n",
              "          ('plea', 'ignor', 'complaint'): 1,\n",
              "          ('ignor', 'complaint', 'pernici'): 1,\n",
              "          ('complaint', 'pernici', 'violenc'): 1,\n",
              "          ('pernici', 'violenc', 'embarrass'): 1,\n",
              "          ('violenc', 'embarrass', 'say'): 1,\n",
              "          ('embarrass', 'say', 'least'): 1,\n",
              "          ('say', 'least', 'haven'): 1,\n",
              "          ('least', 'haven', 'seen'): 1,\n",
              "          ('haven', 'seen', 'comic'): 1,\n",
              "          ('seen', 'comic', 'movi'): 1,\n",
              "          ('comic', 'movi', 'real'): 1,\n",
              "          ('movi', 'real', 'ever'): 1,\n",
              "          ('real', 'ever', 'deserv'): 1,\n",
              "          ('ever', 'deserv', 'better'): 1,\n",
              "          ('deserv', 'better', 'class'): 1,\n",
              "          ('better', 'class', 'crimin'): 1,\n",
              "          ('class', 'crimin', 'phillip'): 1,\n",
              "          ('crimin', 'phillip', 'phoenix'): 1,\n",
              "          ('phillip', 'phoenix', 'deliv'): 1,\n",
              "          ('phoenix', 'deliv', 'dark'): 1,\n",
              "          ('deliv', 'dark', 'joke'): 1,\n",
              "          ('dark', 'joke', 'dark'): 1,\n",
              "          ('joke', 'dark', 'fall'): 1,\n",
              "          ('dark', 'fall', 'love'): 1,\n",
              "          ('fall', 'love', 'villain'): 1,\n",
              "          ('love', 'villain', 'bad'): 1,\n",
              "          ('villain', 'bad', 'gun'): 1,\n",
              "          ('bad', 'gun', 'alway'): 1,\n",
              "          ('gun', 'alway', 'romant'): 1,\n",
              "          ('alway', 'romant', 'anyway'): 1,\n",
              "          ('romant', 'anyway', 'let'): 1,\n",
              "          ('anyway', 'let', 'start'): 1,\n",
              "          ('let', 'start', 'say'): 1,\n",
              "          ('start', 'say', 'joaquin'): 1,\n",
              "          ('say', 'joaquin', 'phoenix'): 1,\n",
              "          ('joaquin', 'phoenix', 'doesn'): 2,\n",
              "          ('phoenix', 'doesn', 'get'): 1,\n",
              "          ('doesn', 'get', 'oscar'): 1,\n",
              "          ('get', 'oscar', 'movi'): 1,\n",
              "          ('oscar', 'movi', 'scar'): 1,\n",
              "          ('movi', 'scar', 'cancel'): 1,\n",
              "          ('scar', 'cancel', 'phoenix'): 1,\n",
              "          ('cancel', 'phoenix', 'amaz'): 1,\n",
              "          ('phoenix', 'amaz', 'might'): 1,\n",
              "          ('amaz', 'might', 'heard'): 1,\n",
              "          ('might', 'heard', 'everi'): 1,\n",
              "          ('heard', 'everi', 'review'): 1,\n",
              "          ('everi', 'review', 'ever'): 1,\n",
              "          ('review', 'ever', 'told'): 1,\n",
              "          ('ever', 'told', 'phillip'): 1,\n",
              "          ('told', 'phillip', 'best'): 1,\n",
              "          ('phillip', 'best', 'stori'): 1,\n",
              "          ('best', 'stori', 'line'): 1,\n",
              "          ('stori', 'line', 'take'): 1,\n",
              "          ('line', 'take', 'visual'): 1,\n",
              "          ('take', 'visual', 'breathtak'): 1,\n",
              "          ('visual', 'breathtak', 'score'): 1,\n",
              "          ('breathtak', 'score', 'org'): 1,\n",
              "          ('score', 'org', 'score'): 1,\n",
              "          ('org', 'score', 'everi'): 1,\n",
              "          ('score', 'everi', 'time'): 1,\n",
              "          ('everi', 'time', 'score'): 1,\n",
              "          ('time', 'score', 'came'): 1,\n",
              "          ('score', 'came', 'felt'): 1,\n",
              "          ('came', 'felt', 'uncomfort'): 1,\n",
              "          ('felt', 'uncomfort', 'like'): 1,\n",
              "          ('uncomfort', 'like', 'someth'): 1,\n",
              "          ('like', 'someth', 'horribl'): 1,\n",
              "          ('someth', 'horribl', 'happen'): 1,\n",
              "          ('horribl', 'happen', 'great'): 1,\n",
              "          ('happen', 'great', 'inspir'): 1,\n",
              "          ('great', 'inspir', 'taxi'): 1,\n",
              "          ('inspir', 'taxi', 'driver'): 2,\n",
              "          ('taxi', 'driver', 'king'): 3,\n",
              "          ('driver', 'king', 'comedi'): 3,\n",
              "          ('king', 'comedi', 'add'): 1,\n",
              "          ('comedi', 'add', 'much'): 1,\n",
              "          ('add', 'much', 'movi'): 1,\n",
              "          ('much', 'movi', 'got'): 1,\n",
              "          ('movi', 'got', 'honest'): 1,\n",
              "          ('got', 'honest', 'scene'): 1,\n",
              "          ('honest', 'scene', 'violent'): 1,\n",
              "          ('scene', 'violent', 'disturb'): 1,\n",
              "          ('violent', 'disturb', 'honestli'): 1,\n",
              "          ('disturb', 'honestli', 'expect'): 1,\n",
              "          ('honestli', 'expect', 'wayyyi'): 1,\n",
              "          ('expect', 'wayyyi', 'violent'): 1,\n",
              "          ('wayyyi', 'violent', 'controversi'): 1,\n",
              "          ('violent', 'controversi', 'go'): 1,\n",
              "          ('controversi', 'go', 'overal'): 1,\n",
              "          ('go', 'overal', 'movi'): 1,\n",
              "          ('overal', 'movi', 'great'): 1,\n",
              "          ('movi', 'great', 'come'): 1,\n",
              "          ('great', 'come', 'oscar'): 1,\n",
              "          ('come', 'oscar', 'season'): 1,\n",
              "          ('oscar', 'season', 'need'): 1,\n",
              "          ('season', 'need', 'nomin'): 1,\n",
              "          ('need', 'nomin', 'best'): 1,\n",
              "          ('nomin', 'best', 'pictur'): 1,\n",
              "          ('best', 'pictur', 'screenplay'): 1,\n",
              "          ('pictur', 'screenplay', 'cinematographi'): 1,\n",
              "          ('screenplay', 'cinematographi', 'actor'): 1,\n",
              "          ('cinematographi', 'actor', 'score'): 1,\n",
              "          ('actor', 'score', 'director'): 1,\n",
              "          ('score', 'director', 'get'): 1,\n",
              "          ('director', 'get', 'peopl'): 1,\n",
              "          ('get', 'peopl', 'hate'): 1,\n",
              "          ('peopl', 'hate', 'polit'): 1,\n",
              "          ('hate', 'polit', 'messag'): 1,\n",
              "          ('polit', 'messag', 'peopl'): 1,\n",
              "          ('messag', 'peopl', 'think'): 1,\n",
              "          ('peopl', 'think', 'need'): 1,\n",
              "          ('think', 'need', 'get'): 1,\n",
              "          ('need', 'get', 'empti'): 1,\n",
              "          ('get', 'empti', 'arthur'): 1,\n",
              "          ('empti', 'arthur', 'mad'): 1,\n",
              "          ('arthur', 'mad', 'come'): 1,\n",
              "          ('mad', 'come', 'point'): 1,\n",
              "          ('come', 'point', 'never'): 1,\n",
              "          ('point', 'never', 'enjoy'): 1,\n",
              "          ('never', 'enjoy', 'masterpiec'): 1,\n",
              "          ('enjoy', 'masterpiec', 'joaquin'): 1,\n",
              "          ('masterpiec', 'joaquin', 'phoenix'): 2,\n",
              "          ('joaquin', 'phoenix', 'told'): 1,\n",
              "          ('phoenix', 'told', 'phillip'): 1,\n",
              "          ('told', 'phillip', 'overdid'): 1,\n",
              "          ('phillip', 'overdid', 'movi'): 1,\n",
              "          ('overdid', 'movi', 'actingmus'): 1,\n",
              "          ('movi', 'actingmus', 'cinematographi'): 1,\n",
              "          ('actingmus', 'cinematographi', 'amaz'): 1,\n",
              "          ('cinematographi', 'amaz', 'plea'): 1,\n",
              "          ('amaz', 'plea', 'enjoy'): 1,\n",
              "          ('plea', 'enjoy', 'movi'): 1,\n",
              "          ('enjoy', 'movi', 'without'): 1,\n",
              "          ('movi', 'without', 'overthink'): 1,\n",
              "          ('without', 'overthink', 'seen'): 1,\n",
              "          ('overthink', 'seen', 'joke'): 1,\n",
              "          ('seen', 'joke', 'yesterday'): 1,\n",
              "          ('joke', 'yesterday', 'venic'): 1,\n",
              "          ('yesterday', 'venic', 'earli'): 1,\n",
              "          ('venic', 'earli', 'inflat'): 1,\n",
              "          ('earli', 'inflat', 'screen'): 1,\n",
              "          ('inflat', 'screen', 'troubl'): 1,\n",
              "          ('screen', 'troubl', 'audit'): 1,\n",
              "          ('troubl', 'audit', 'lead'): 1,\n",
              "          ('audit', 'lead', 'nearhour'): 1,\n",
              "          ('lead', 'nearhour', 'delay'): 1,\n",
              "          ('nearhour', 'delay', 'definit'): 1,\n",
              "          ('delay', 'definit', 'worth'): 1,\n",
              "          ('definit', 'worth', 'joke'): 1,\n",
              "          ('worth', 'joke', 'deserv'): 1,\n",
              "          ('joke', 'deserv', 'present'): 1,\n",
              "          ('deserv', 'present', 'venic'): 1,\n",
              "          ('present', 'venic', 'film'): 1,\n",
              "          ('venic', 'film', 'festiv'): 1,\n",
              "          ('film', 'festiv', 'event'): 1,\n",
              "          ('festiv', 'event', 'regard'): 1,\n",
              "          ('event', 'regard', 'cinema'): 1,\n",
              "          ('regard', 'cinema', 'form'): 1,\n",
              "          ('cinema', 'form', 'art'): 1,\n",
              "          ('form', 'art', 'film'): 1,\n",
              "          ('art', 'film', 'far'): 1,\n",
              "          ('film', 'far', 'blockbust'): 1,\n",
              "          ('far', 'blockbust', 'mere'): 1,\n",
              "          ('blockbust', 'mere', 'entertain'): 1,\n",
              "          ('mere', 'entertain', 'movi'): 1,\n",
              "          ('entertain', 'movi', 'film'): 1,\n",
              "          ('movi', 'film', 'gene'): 1,\n",
              "          ('film', 'gene', 'focus'): 1,\n",
              "          ('gene', 'focus', 'psychic'): 1,\n",
              "          ('focus', 'psychic', 'main'): 1,\n",
              "          ('psychic', 'main', 'charact'): 1,\n",
              "          ('main', 'charact', 'slowli'): 1,\n",
              "          ('charact', 'slowli', 'crumbl'): 1,\n",
              "          ('slowli', 'crumbl', 'pressur'): 1,\n",
              "          ('crumbl', 'pressur', 'societi'): 1,\n",
              "          ('pressur', 'societi', 'thu'): 1,\n",
              "          ('societi', 'thu', 'joaquin'): 1,\n",
              "          ('thu', 'joaquin', 'phoenix'): 1,\n",
              "          ('joaquin', 'phoenix', 'wonder'): 1,\n",
              "          ('phoenix', 'wonder', 'perform'): 1,\n",
              "          ('wonder', 'perform', 'earn'): 1,\n",
              "          ('perform', 'earn', 'almost'): 1,\n",
              "          ('earn', 'almost', 'sure'): 1,\n",
              "          ('almost', 'sure', 'nomin'): 1,\n",
              "          ('sure', 'nomin', 'scar'): 1,\n",
              "          ('nomin', 'scar', 'least'): 1,\n",
              "          ('scar', 'least', 'take'): 1,\n",
              "          ('least', 'take', 'joke'): 1,\n",
              "          ('take', 'joke', 'differ'): 1,\n",
              "          ('joke', 'differ', 'ledger'): 1,\n",
              "          ('differ', 'ledger', 'id'): 1,\n",
              "          ('ledger', 'id', 'say'): 1,\n",
              "          ('id', 'say', 'equal'): 1,\n",
              "          ('say', 'equal', 'good'): 1,\n",
              "          ('equal', 'good', 'main'): 1,\n",
              "          ('good', 'main', 'differ'): 1,\n",
              "          ('main', 'differ', 'might'): 1,\n",
              "          ('differ', 'might', 'ledger'): 1,\n",
              "          ('might', 'ledger', 'joke'): 1,\n",
              "          ('ledger', 'joke', 'ration'): 1,\n",
              "          ('joke', 'ration', 'act'): 1,\n",
              "          ('ration', 'act', 'insan'): 1,\n",
              "          ('act', 'insan', 'phoenix'): 1,\n",
              "          ('insan', 'phoenix', 'insan'): 1,\n",
              "          ('phoenix', 'insan', 'root'): 1,\n",
              "          ('insan', 'root', 'despit'): 1,\n",
              "          ('root', 'despit', 'movi'): 1,\n",
              "          ('despit', 'movi', 'superhero'): 1,\n",
              "          ('movi', 'superhero', 'villain'): 1,\n",
              "          ('superhero', 'villain', 'joke'): 1,\n",
              "          ('villain', 'joke', 'much'): 1,\n",
              "          ('joke', 'much', 'superior'): 1,\n",
              "          ('much', 'superior', 'move'): 1,\n",
              "          ('superior', 'move', 'gene'): 1,\n",
              "          ('move', 'gene', 'id'): 1,\n",
              "          ('gene', 'id', 'exclud'): 1,\n",
              "          ('id', 'exclud', 'dark'): 1,\n",
              "          ('exclud', 'dark', 'knight'): 1,\n",
              "          ('dark', 'knight', 'trilog'): 2,\n",
              "          ('knight', 'trilog', 'joke'): 1,\n",
              "          ('trilog', 'joke', 'easili'): 1,\n",
              "          ('joke', 'easili', 'good'): 1,\n",
              "          ('easili', 'good', 'plan'): 1,\n",
              "          ('good', 'plan', 'move'): 1,\n",
              "          ('plan', 'move', 'least'): 1,\n",
              "          ('move', 'least', 'close'): 1,\n",
              "          ('least', 'close', 'smallscal'): 1,\n",
              "          ('close', 'smallscal', 'film'): 1,\n",
              "          ('smallscal', 'film', 'distinct'): 1,\n",
              "          ('film', 'distinct', 'style'): 1,\n",
              "          ('distinct', 'style', 'cinematographi'): 1,\n",
              "          ('style', 'cinematographi', 'can'): 1,\n",
              "          ('cinematographi', 'can', 'not'): 1,\n",
              "          ('can', 'not', 'appreci'): 1,\n",
              "          ('not', 'appreci', 'set'): 1,\n",
              "          ('appreci', 'set', 'cinephil'): 1,\n",
              "          ('set', 'cinephil', 'refer'): 1,\n",
              "          ('cinephil', 'refer', 'howev'): 1,\n",
              "          ('refer', 'howev', 'feel'): 1,\n",
              "          ('howev', 'feel', 'forc'): 1,\n",
              "          ('feel', 'forc', 'overlay'): 1,\n",
              "          ('forc', 'overlay', 'oppress'): 1,\n",
              "          ('overlay', 'oppress', 'notabl'): 1,\n",
              "          ('oppress', 'notabl', 'similar'): 1,\n",
              "          ('notabl', 'similar', 'scorses'): 1,\n",
              "          ('similar', 'scorses', 'taxi'): 1,\n",
              "          ('scorses', 'taxi', 'driver'): 1,\n",
              "          ('king', 'comedi', 'also'): 1,\n",
              "          ('comedi', 'also', 'chain'): 1,\n",
              "          ('also', 'chain', 'modern'): 1,\n",
              "          ('chain', 'modern', 'time'): 1,\n",
              "          ('modern', 'time', 'somewhat'): 1,\n",
              "          ('time', 'somewhat', 'refer'): 1,\n",
              "          ('somewhat', 'refer', 'eager'): 1,\n",
              "          ('refer', 'eager', 'see'): 1,\n",
              "          ('eager', 'see', 'noncomed'): 1,\n",
              "          ('see', 'noncomed', 'effort'): 1,\n",
              "          ('noncomed', 'effort', 'told'): 1,\n",
              "          ('effort', 'told', 'phillip'): 1,\n",
              "          ('told', 'phillip', 'movi'): 1,\n",
              "          ('phillip', 'movi', 'far'): 1,\n",
              "          ('movi', 'far', 'probabl'): 1,\n",
              "          ('far', 'probabl', 'best'): 1,\n",
              "          ('probabl', 'best', '2019'): 1,\n",
              "          ('best', '2019', 'worst'): 1,\n",
              "          ('2019', 'worst', 'contest'): 1,\n",
              "          ('worst', 'contest', 'far'): 1,\n",
              "          ('contest', 'far', 'door'): 1,\n",
              "          ('far', 'door', 'gloria'): 1,\n",
              "          ('door', 'gloria', 'upon'): 1,\n",
              "          ('gloria', 'upon', 'time'): 1,\n",
              "          ('upon', 'time', 'hollywood'): 2,\n",
              "          ('time', 'hollywood', 'convinc'): 1,\n",
              "          ('hollywood', 'convinc', 'sad'): 1,\n",
              "          ('convinc', 'sad', 'joaquin'): 1,\n",
              "          ('sad', 'joaquin', 'miss'): 1,\n",
              "          ('joaquin', 'miss', 'oscar'): 1,\n",
              "          ('miss', 'oscar', 'gladiat'): 1,\n",
              "          ('oscar', 'gladiat', 'compel'): 1,\n",
              "          ('gladiat', 'compel', 'villain'): 1,\n",
              "          ('compel', 'villain', 'quit'): 1,\n",
              "          ('villain', 'quit', 'confid'): 1,\n",
              "          ('quit', 'confid', 'win'): 1,\n",
              "          ('confid', 'win', 'joke'): 1,\n",
              "          ('win', 'joke', 'damn'): 1,\n",
              "          ('joke', 'damn', 'movi'): 1,\n",
              "          ('damn', 'movi', 'keep'): 1,\n",
              "          ('movi', 'keep', 'u'): 1,\n",
              "          ('keep', 'u', 'toe'): 1,\n",
              "          ('u', 'toe', 'time'): 1,\n",
              "          ('toe', 'time', 'predict'): 1,\n",
              "          ('time', 'predict', 'storylin'): 1,\n",
              "          ('predict', 'storylin', 'realli'): 1,\n",
              "          ('storylin', 'realli', 'deep'): 1,\n",
              "          ('realli', 'deep', 'interest'): 1,\n",
              "          ('deep', 'interest', 'plot'): 1,\n",
              "          ('interest', 'plot', 'forget'): 1,\n",
              "          ('plot', 'forget', 'mention'): 1,\n",
              "          ('forget', 'mention', 'act'): 1,\n",
              "          ('mention', 'act', 'damn'): 1,\n",
              "          ('act', 'damn', 'no'): 1,\n",
              "          ('damn', 'no', 'joaquin'): 1,\n",
              "          ('no', 'joaquin', 'teach'): 1,\n",
              "          ('joaquin', 'teach', 'u'): 1,\n",
              "          ('teach', 'u', 'realli'): 1,\n",
              "          ('u', 'realli', '5'): 1,\n",
              "          ('realli', '5', 'star'): 1,\n",
              "          ('5', 'star', 'act'): 1,\n",
              "          ('star', 'act', 'enjoy'): 1,\n",
              "          ('act', 'enjoy', 'movi'): 1,\n",
              "          ('enjoy', 'movi', 'get'): 1,\n",
              "          ('movi', 'get', 'wine'): 1,\n",
              "          ('get', 'wine', 'hand'): 1,\n",
              "          ('wine', 'hand', 'close'): 1,\n",
              "          ('hand', 'close', 'curtain'): 1,\n",
              "          ('close', 'curtain', 'turn'): 1,\n",
              "          ('curtain', 'turn', 'or'): 1,\n",
              "          ('turn', 'or', 'cellphon'): 1,\n",
              "          ('or', 'cellphon', 'put'): 1,\n",
              "          ('cellphon', 'put', 'disturb'): 1,\n",
              "          ('put', 'disturb', 'sign'): 1,\n",
              "          ('disturb', 'sign', 'or'): 1,\n",
              "          ('sign', 'or', 'door'): 1,\n",
              "          ('or', 'door', 'best'): 1,\n",
              "          ('door', 'best', 'dark'): 1,\n",
              "          ('best', 'dark', 'thrill'): 1,\n",
              "          ('dark', 'thrill', 'suspens'): 1,\n",
              "          ('thrill', 'suspens', 'movi'): 1,\n",
              "          ('suspens', 'movi', 'get'): 1,\n",
              "          ('movi', 'get', 'experi'): 1,\n",
              "          ('get', 'experi', 'movi'): 1,\n",
              "          ('experi', 'movi', 'caus'): 1,\n",
              "          ('movi', 'caus', 'audienc'): 1,\n",
              "          ('caus', 'audienc', 'consid'): 1,\n",
              "          ('audienc', 'consid', 'mani'): 1,\n",
              "          ('consid', 'mani', 'topic'): 1,\n",
              "          ('mani', 'topic', 'moral'): 1,\n",
              "          ('topic', 'moral', 'particularli'): 1,\n",
              "          ('moral', 'particularli', 'poor'): 1,\n",
              "          ('particularli', 'poor', 'choic'): 1,\n",
              "          ('poor', 'choic', 'made'): 1,\n",
              "          ('choic', 'made', 'joke'): 1,\n",
              "          ('made', 'joke', 'joke'): 1,\n",
              "          ('joke', 'joke', 'reliabl'): 1,\n",
              "          ('joke', 'reliabl', 'movi'): 1,\n",
              "          ('reliabl', 'movi', 'antagonist'): 1,\n",
              "          ('movi', 'antagonist', 'usual'): 1,\n",
              "          ('antagonist', 'usual', 'think'): 1,\n",
              "          ('usual', 'think', 'give'): 1,\n",
              "          ('think', 'give', 'realist'): 1,\n",
              "          ('give', 'realist', 'view'): 1,\n",
              "          ('realist', 'view', 'bad'): 1,\n",
              "          ('view', 'bad', 'peopl'): 1,\n",
              "          ('bad', 'peopl', 'usual'): 1,\n",
              "          ('peopl', 'usual', '100'): 1,\n",
              "          ('usual', '100', 'bad'): 1,\n",
              "          ('100', 'bad', 'way'): 1,\n",
              "          ('bad', 'way', 'societi'): 1,\n",
              "          ('way', 'societi', 'often'): 1,\n",
              "          ('societi', 'often', 'judg'): 1,\n",
              "          ('often', 'judg', 'import'): 1,\n",
              "          ('judg', 'import', 'know'): 1,\n",
              "          ('import', 'know', 'sympath'): 1,\n",
              "          ('know', 'sympath', 'joke'): 1,\n",
              "          ('sympath', 'joke', 'toward'): 1,\n",
              "          ('joke', 'toward', 'begin'): 1,\n",
              "          ('toward', 'begin', 'movi'): 1,\n",
              "          ('begin', 'movi', 'need'): 1,\n",
              "          ('movi', 'need', 'identifi'): 1,\n",
              "          ('need', 'identifi', 'threat'): 1,\n",
              "          ('identifi', 'threat', 'begin'): 1,\n",
              "          ('threat', 'begin', 'make'): 1,\n",
              "          ('begin', 'make', 'certain'): 1,\n",
              "          ('make', 'certain', 'bad'): 1,\n",
              "          ('certain', 'bad', 'choic'): 1,\n",
              "          ('bad', 'choic', 'movi'): 1,\n",
              "          ('choic', 'movi', 'suitabl'): 1,\n",
              "          ('movi', 'suitabl', 'adult'): 1,\n",
              "          ('suitabl', 'adult', 'will'): 1,\n",
              "          ('adult', 'will', 'pick'): 1,\n",
              "          ('will', 'pick', 'apart'): 1,\n",
              "          ('pick', 'apart', 'think'): 1,\n",
              "          ('apart', 'think', 'clinic'): 1,\n",
              "          ('think', 'clinic', 'isn'): 1,\n",
              "          ('clinic', 'isn', 'light'): 1,\n",
              "          ('isn', 'light', 'amus'): 1,\n",
              "          ('light', 'amus', 'substanc'): 1,\n",
              "          ('amus', 'substanc', 'make'): 1,\n",
              "          ('substanc', 'make', 'good'): 1,\n",
              "          ('make', 'good', 'agent'): 1,\n",
              "          ('good', 'agent', 'abl'): 1,\n",
              "          ('agent', 'abl', 'ponder'): 1,\n",
              "          ('abl', 'ponder', 'underli'): 1,\n",
              "          ('ponder', 'underli', 'theme'): 1,\n",
              "          ('underli', 'theme', 'hand'): 1,\n",
              "          ('theme', 'hand', 'thought'): 1,\n",
              "          ('hand', 'thought', 'isn'): 1,\n",
              "          ('thought', 'isn', 'best'): 1,\n",
              "          ('isn', 'best', 'movi'): 1,\n",
              "          ('best', 'movi', 'act'): 1,\n",
              "          ('movi', 'act', 'cinematographi'): 1,\n",
              "          ('act', 'cinematographi', 'sound'): 1,\n",
              "          ('cinematographi', 'sound', 'design'): 1,\n",
              "          ('sound', 'design', 'script'): 1,\n",
              "          ('design', 'script', 'phenomenon'): 1,\n",
              "          ('script', 'phenomenon', 'movi'): 1,\n",
              "          ('phenomenon', 'movi', 'triumph'): 1,\n",
              "          ('movi', 'triumph', 'joaquin'): 1,\n",
              "          ('triumph', 'joaquin', 'phoenix'): 1,\n",
              "          ('joaquin', 'phoenix', 'deserv'): 2,\n",
              "          ('phoenix', 'deserv', 'oscar'): 2,\n",
              "          ('deserv', 'oscar', 'win'): 1,\n",
              "          ('oscar', 'win', 'th'): 1,\n",
              "          ('win', 'th', 'movi'): 1,\n",
              "          ('th', 'movi', 'affect'): 1,\n",
              "          ('movi', 'affect', 'way'): 1,\n",
              "          ('affect', 'way', 'make'): 1,\n",
              "          ('way', 'make', 'physic'): 1,\n",
              "          ('make', 'physic', 'pain'): 1,\n",
              "          ('physic', 'pain', 'experi'): 1,\n",
              "          ('pain', 'experi', 'good'): 1,\n",
              "          ('experi', 'good', 'way'): 1,\n",
              "          ('good', 'way', 'need'): 1,\n",
              "          ('way', 'need', 'say'): 1,\n",
              "          ('need', 'say', 'everyth'): 1,\n",
              "          ('say', 'everyth', 'movi'): 1,\n",
              "          ('everyth', 'movi', 'wast'): 1,\n",
              "          ('movi', 'wast', 'everi'): 1,\n",
              "          ('wast', 'everi', 'singl'): 1,\n",
              "          ('everi', 'singl', 'way'): 1,\n",
              "          ('singl', 'way', 'joke'): 1,\n",
              "          ('way', 'joke', 'isn'): 1,\n",
              "          ('joke', 'isn', 'awesom'): 1,\n",
              "          ('isn', 'awesom', 'comic'): 1,\n",
              "          ('awesom', 'comic', 'book'): 1,\n",
              "          ('comic', 'book', 'movi'): 2,\n",
              "          ('book', 'movi', 'awesom'): 1,\n",
              "          ('movi', 'awesom', 'movi'): 1,\n",
              "          ('awesom', 'movi', 'period'): 1,\n",
              "          ('movi', 'period', 'offer'): 1,\n",
              "          ('period', 'offer', 'easi'): 1,\n",
              "          ('offer', 'easi', 'answer'): 1,\n",
              "          ('easi', 'answer', 'settl'): 1,\n",
              "          ('answer', 'settl', 'question'): 1,\n",
              "          ('settl', 'question', 'rais'): 1,\n",
              "          ('question', 'rais', 'cruel'): 1,\n",
              "          ('rais', 'cruel', 'societi'): 1,\n",
              "          ('cruel', 'societi', 'declin'): 1,\n",
              "          ('societi', 'declin', 'joaquin'): 1,\n",
              "          ('declin', 'joaquin', 'phoenix'): 1,\n",
              "          ('joaquin', 'phoenix', 'fulli'): 1,\n",
              "          ('phoenix', 'fulli', 'commit'): 1,\n",
              "          ('fulli', 'commit', 'perform'): 1,\n",
              "          ('commit', 'perform', 'told'): 1,\n",
              "          ('perform', 'told', 'phillip'): 1,\n",
              "          ('told', 'phillip', 'wast'): 1,\n",
              "          ('phillip', 'wast', 'albeit'): 1,\n",
              "          ('wast', 'albeit', 'loo'): 1,\n",
              "          ('albeit', 'loo', 'invent'): 1,\n",
              "          ('loo', 'invent', 'do'): 1,\n",
              "          ('invent', 'do', 'sourc'): 1,\n",
              "          ('do', 'sourc', 'materi'): 1,\n",
              "          ('sourc', 'materi', 'make'): 1,\n",
              "          ('materi', 'make', 'joke'): 1,\n",
              "          ('make', 'joke', 'film'): 1,\n",
              "          ('joke', 'film', 'leav'): 1,\n",
              "          ('film', 'leav', 'comic'): 1,\n",
              "          ('leav', 'comic', 'book'): 1,\n",
              "          ('comic', 'book', 'fan'): 1,\n",
              "          ('book', 'fan', 'nonfan'): 1,\n",
              "          ('fan', 'nonfan', 'alik'): 1,\n",
              "          ('nonfan', 'alik', 'disturb'): 1,\n",
              "          ('alik', 'disturb', 'move'): 1,\n",
              "          ('disturb', 'move', 'right'): 1,\n",
              "          ('move', 'right', 'way'): 1,\n",
              "          ('right', 'way', 'quit'): 1,\n",
              "          ('way', 'quit', 'reli'): 1,\n",
              "          ('quit', 'reli', 'critic'): 1,\n",
              "          ('reli', 'critic', 'review'): 1,\n",
              "          ('critic', 'review', 'year'): 1,\n",
              "          ('review', 'year', 'ago'): 1,\n",
              "          ('year', 'ago', 'notic'): 1,\n",
              "          ('ago', 'notic', 'movi'): 1,\n",
              "          ('notic', 'movi', 'critic'): 1,\n",
              "          ('movi', 'critic', 'rate'): 1,\n",
              "          ('critic', 'rate', 'low'): 1,\n",
              "          ('rate', 'low', 'viewer'): 1,\n",
              "          ('low', 'viewer', 'rate'): 1,\n",
              "          ('viewer', 'rate', 'high'): 1,\n",
              "          ('rate', 'high', 'said'): 1,\n",
              "          ('high', 'said', 'point'): 1,\n",
              "          ('said', 'point', 'critic'): 1,\n",
              "          ('point', 'critic', 'movi'): 1,\n",
              "          ('critic', 'movi', 'oh'): 1,\n",
              "          ('movi', 'oh', 'wrong'): 1,\n",
              "          ('oh', 'wrong', 'went'): 1,\n",
              "          ('wrong', 'went', 'saw'): 1,\n",
              "          ('went', 'saw', 'tone'): 1,\n",
              "          ('saw', 'tone', 'mon'): 1,\n",
              "          ('tone', 'mon', 'went'): 1,\n",
              "          ('mon', 'went', 'skeptic'): 1,\n",
              "          ('went', 'skeptic', 'w'): 1,\n",
              "          ('skeptic', 'w', 'low'): 1,\n",
              "          ('w', 'low', 'expect'): 1,\n",
              "          ('low', 'expect', 'knew'): 1,\n",
              "          ('expect', 'knew', 'expect'): 1,\n",
              "          ('knew', 'expect', 'typic'): 1,\n",
              "          ('expect', 'typic', 'super'): 1,\n",
              "          ('typic', 'super', 'hero'): 1,\n",
              "          ('super', 'hero', 'movi'): 2,\n",
              "          ('hero', 'movi', 'got'): 1,\n",
              "          ('movi', 'got', 'masterpiec'): 1,\n",
              "          ('got', 'masterpiec', 'went'): 1,\n",
              "          ('masterpiec', 'went', 'expect'): 1,\n",
              "          ('went', 'expect', 'sheer'): 1,\n",
              "          ('expect', 'sheer', 'boredom'): 1,\n",
              "          ('sheer', 'boredom', 'instead'): 1,\n",
              "          ('boredom', 'instead', 'edg'): 1,\n",
              "          ('instead', 'edg', 'seat'): 1,\n",
              "          ('edg', 'seat', 'whole'): 1,\n",
              "          ('seat', 'whole', 'time'): 1,\n",
              "          ('whole', 'time', 'action'): 1,\n",
              "          ('time', 'action', 'edg'): 1,\n",
              "          ('action', 'edg', 'seat'): 1,\n",
              "          ('edg', 'seat', 'fear'): 1,\n",
              "          ('seat', 'fear', 'may'): 1,\n",
              "          ('fear', 'may', 'say'): 1,\n",
              "          ('may', 'say', 'next'): 1,\n",
              "          ('say', 'next', 'edg'): 1,\n",
              "          ('next', 'edg', 'seat'): 1,\n",
              "          ('edg', 'seat', 'see'): 1,\n",
              "          ('seat', 'see', 'charact'): 1,\n",
              "          ('see', 'charact', 'evolv'): 1,\n",
              "          ('charact', 'evolv', 'know'): 1,\n",
              "          ('evolv', 'know', '1'): 1,\n",
              "          ('know', '1', 'villain'): 1,\n",
              "          ('1', 'villain', 'batman'): 1,\n",
              "          ('villain', 'batman', 'beyond'): 1,\n",
              "          ('batman', 'beyond', 'geniu'): 1,\n",
              "          ('beyond', 'geniu', 'joaquin'): 1,\n",
              "          ('geniu', 'joaquin', 'phoenix'): 1,\n",
              "          ('joaquin', 'phoenix', 'true'): 1,\n",
              "          ('phoenix', 'true', 'master'): 1,\n",
              "          ('true', 'master', 'perfect'): 1,\n",
              "          ('master', 'perfect', 'art'): 1,\n",
              "          ('perfect', 'art', 'consum'): 1,\n",
              "          ('art', 'consum', 'charact'): 1,\n",
              "          ('consum', 'charact', 'may'): 1,\n",
              "          ('charact', 'may', 'place'): 1,\n",
              "          ('may', 'place', 'dont'): 1,\n",
              "          ('place', 'dont', 'care'): 1,\n",
              "          ('dont', 'care', 'movi'): 1,\n",
              "          ('care', 'movi', 'stori'): 1,\n",
              "          ('movi', 'stori', 'act'): 1,\n",
              "          ('stori', 'act', 'direct'): 1,\n",
              "          ('act', 'direct', 'effect'): 1,\n",
              "          ('direct', 'effect', 'everyon'): 1,\n",
              "          ('effect', 'everyon', 'brought'): 1,\n",
              "          ('everyon', 'brought', 'game'): 1,\n",
              "          ('brought', 'game', 'up'): 1,\n",
              "          ('game', 'up', 'definit'): 1,\n",
              "          ('up', 'definit', 'deserv'): 1,\n",
              "          ('definit', 'deserv', 'nod'): 1,\n",
              "          ('deserv', 'nod', 'win'): 1,\n",
              "          ('nod', 'win', 'role'): 1,\n",
              "          ('win', 'role', 'comparison'): 1,\n",
              "          ('role', 'comparison', 'jack'): 1,\n",
              "          ('comparison', 'jack', 'nicholson'): 1,\n",
              "          ('jack', 'nicholson', 'heath'): 3,\n",
              "          ('nicholson', 'heath', 'ledger'): 3,\n",
              "          ('heath', 'ledger', 'actor'): 1,\n",
              "          ('ledger', 'actor', 'took'): 1,\n",
              "          ('actor', 'took', 'charact'): 1,\n",
              "          ('took', 'charact', 'differ'): 1,\n",
              "          ('charact', 'differ', 'level'): 1,\n",
              "          ('differ', 'level', 'differ'): 1,\n",
              "          ('level', 'differ', 'stori'): 1,\n",
              "          ('differ', 'stori', 'point'): 1,\n",
              "          ('stori', 'point', 'differ'): 1,\n",
              "          ('point', 'differ', 'point'): 1,\n",
              "          ('differ', 'point', 'time'): 1,\n",
              "          ('point', 'time', 'comparison'): 1,\n",
              "          ('time', 'comparison', 'would'): 1,\n",
              "          ('comparison', 'would', 'feasibl'): 1,\n",
              "          ('would', 'feasibl', 'fair'): 1,\n",
              "          ('feasibl', 'fair', 'actor'): 1,\n",
              "          ('fair', 'actor', 'embrac'): 1,\n",
              "          ('actor', 'embrac', 'made'): 1,\n",
              "          ('embrac', 'made', 'come'): 1,\n",
              "          ('made', 'come', 'life'): 1,\n",
              "          ('come', 'life', 'geniu'): 1,\n",
              "          ('life', 'geniu', 'way'): 1,\n",
              "          ('geniu', 'way', 'stop'): 1,\n",
              "          ('way', 'stop', 'watch'): 1,\n",
              "          ('stop', 'watch', 'move'): 1,\n",
              "          ('watch', 'move', 'joaquin'): 1,\n",
              "          ('move', 'joaquin', 'phoenix'): 1,\n",
              "          ('joaquin', 'phoenix', 'award'): 1,\n",
              "          ('phoenix', 'award', 'oscar'): 1,\n",
              "          ('award', 'oscar', 'tremend'): 1,\n",
              "          ('oscar', 'tremend', 'act'): 1,\n",
              "          ('tremend', 'act', 'thought'): 1,\n",
              "          ('act', 'thought', 'film'): 1,\n",
              "          ('thought', 'film', 'good'): 1,\n",
              "          ('film', 'good', 'dont'): 1,\n",
              "          ('good', 'dont', 'get'): 1,\n",
              "          ('dont', 'get', 'hope'): 1,\n",
              "          ('get', 'hope', 'person'): 1,\n",
              "          ('hope', 'person', 'act'): 1,\n",
              "          ('person', 'act', 'amaz'): 1,\n",
              "          ('act', 'amaz', 'film'): 1,\n",
              "          ('amaz', 'film', 'good'): 1,\n",
              "          ('film', 'good', 'overal'): 1,\n",
              "          ('good', 'overal', 'think'): 1,\n",
              "          ('overal', 'think', 'masterpiec'): 1,\n",
              "          ('think', 'masterpiec', 'film'): 1,\n",
              "          ('masterpiec', 'film', 'year'): 1,\n",
              "          ('film', 'year', 'bit'): 1,\n",
              "          ('year', 'bit', 'vers'): 1,\n",
              "          ('bit', 'vers', 'throughout'): 1,\n",
              "          ('vers', 'throughout', 'review'): 1,\n",
              "          ('throughout', 'review', 'way'): 1,\n",
              "          ('review', 'way', 'dislik'): 1,\n",
              "          ('way', 'dislik', 'film'): 1,\n",
              "          ('dislik', 'film', 'thought'): 1,\n",
              "          ('film', 'thought', 'realli'): 1,\n",
              "          ('thought', 'realli', 'good'): 1,\n",
              "          ('realli', 'good', 'overhyp'): 1,\n",
              "          ('good', 'overhyp', 'feel'): 1,\n",
              "          ('overhyp', 'feel', 'though'): 1,\n",
              "          ('feel', 'though', 'lot'): 1,\n",
              "          ('though', 'lot', '1010'): 1,\n",
              "          ('lot', '1010', 'review'): 1,\n",
              "          ('1010', 'review', 'pure'): 1,\n",
              "          ('review', 'pure', 'base'): 1,\n",
              "          ('pure', 'base', 'fact'): 1,\n",
              "          ('base', 'fact', 'alreadi'): 1,\n",
              "          ('fact', 'alreadi', 'amaz'): 1,\n",
              "          ('alreadi', 'amaz', 'review'): 1,\n",
              "          ('amaz', 'review', 'peopl'): 1,\n",
              "          ('review', 'peopl', 'want'): 1,\n",
              "          ('peopl', 'want', 'carri'): 1,\n",
              "          ('want', 'carri', 'prais'): 1,\n",
              "          ('carri', 'prais', 'joke'): 1,\n",
              "          ('prais', 'joke', 'film'): 1,\n",
              "          ('joke', 'film', 'releas'): 1,\n",
              "          ('film', 'releas', 'random'): 1,\n",
              "          ('releas', 'random', 'guy'): 1,\n",
              "          ('random', 'guy', 'situat'): 1,\n",
              "          ('guy', 'situat', 'dont'): 1,\n",
              "          ('situat', 'dont', 'think'): 1,\n",
              "          ('dont', 'think', 'review'): 1,\n",
              "          ('think', 'review', 'would'): 1,\n",
              "          ('review', 'would', 'high'): 1,\n",
              "          ('would', 'high', 'mayb'): 1,\n",
              "          ('high', 'mayb', 'that'): 1,\n",
              "          ('mayb', 'that', 'point'): 1,\n",
              "          ('that', 'point', 'realli'): 1,\n",
              "          ('point', 'realli', 'understand'): 1,\n",
              "          ('realli', 'understand', 'ten'): 1,\n",
              "          ('understand', 'ten', 'sure'): 1,\n",
              "          ('ten', 'sure', 'good'): 1,\n",
              "          ('sure', 'good', 'movi'): 1,\n",
              "          ('good', 'movi', 'good'): 1,\n",
              "          ('movi', 'good', 'act'): 1,\n",
              "          ('good', 'act', 'nice'): 1,\n",
              "          ('act', 'nice', 'shoot'): 1,\n",
              "          ('nice', 'shoot', 'plot'): 1,\n",
              "          ('shoot', 'plot', 'intrigu'): 1,\n",
              "          ('plot', 'intrigu', 'time'): 1,\n",
              "          ('intrigu', 'time', 'realli'): 1,\n",
              "          ('time', 'realli', 'long'): 1,\n",
              "          ('realli', 'long', 'somewhat'): 1,\n",
              "          ('long', 'somewhat', 'bore'): 1,\n",
              "          ('somewhat', 'bore', 'need'): 1,\n",
              "          ('bore', 'need', 'doublecheck'): 1,\n",
              "          ('need', 'doublecheck', 'gene'): 1,\n",
              "          ('doublecheck', 'gene', 'see'): 1,\n",
              "          ('gene', 'see', 'drama'): 1,\n",
              "          ('see', 'drama', 'crime'): 1,\n",
              "          ('drama', 'crime', 'drama'): 1,\n",
              "          ('crime', 'drama', 'thrill'): 1,\n",
              "          ('drama', 'thrill', 'said'): 1,\n",
              "          ('thrill', 'said', 'would'): 1,\n",
              "          ('said', 'would', 'put'): 1,\n",
              "          ('would', 'put', 'mayb'): 1,\n",
              "          ('put', 'mayb', '90'): 1,\n",
              "          ('mayb', '90', 'drama'): 1,\n",
              "          ('90', 'drama', 'rest'): 1,\n",
              "          ('drama', 'rest', 'thrill'): 1,\n",
              "          ('rest', 'thrill', 'miss'): 1,\n",
              "          ('thrill', 'miss', 'dialogu'): 1,\n",
              "          ('miss', 'dialogu', 'depth'): 1,\n",
              "          ('dialogu', 'depth', 'substanc'): 1,\n",
              "          ('depth', 'substanc', 'plot'): 1,\n",
              "          ('substanc', 'plot', 'realli'): 1,\n",
              "          ('plot', 'realli', 'somewhat'): 1,\n",
              "          ('realli', 'somewhat', 'crazi'): 1,\n",
              "          ('somewhat', 'crazi', 'person'): 1,\n",
              "          ('crazi', 'person', 'becom'): 1,\n",
              "          ('person', 'becom', 'bit'): 1,\n",
              "          ('becom', 'bit', 'brazier'): 1,\n",
              "          ('bit', 'brazier', 'sure'): 1,\n",
              "          ('brazier', 'sure', 'underton'): 1,\n",
              "          ('sure', 'underton', 'societi'): 1,\n",
              "          ('underton', 'societi', 'system'): 1,\n",
              "          ('societi', 'system', 'mean'): 1,\n",
              "          ('system', 'mean', 'charact'): 1,\n",
              "          ('mean', 'charact', 'peopl'): 1,\n",
              "          ('charact', 'peopl', 'bring'): 1,\n",
              "          ('peopl', 'bring', 'creat'): 1,\n",
              "          ('bring', 'creat', 'evil'): 1,\n",
              "          ('creat', 'evil', 'beast'): 1,\n",
              "          ('evil', 'beast', 'later'): 1,\n",
              "          ('beast', 'later', 'want'): 1,\n",
              "          ('later', 'want', 'destroy'): 1,\n",
              "          ('want', 'destroy', 'everyth'): 1,\n",
              "          ('destroy', 'everyth', 'absolut'): 1,\n",
              "          ('everyth', 'absolut', 'think'): 1,\n",
              "          ('absolut', 'think', 'intrigu'): 1,\n",
              "          ('think', 'intrigu', 'stori'): 1,\n",
              "          ('intrigu', 'stori', 'plot'): 1,\n",
              "          ('stori', 'plot', 'dialogu'): 1,\n",
              "          ('plot', 'dialogu', 'enough'): 1,\n",
              "          ('dialogu', 'enough', 'give'): 1,\n",
              "          ('enough', 'give', 'ten'): 1,\n",
              "          ('give', 'ten', 'worthi'): 1,\n",
              "          ('ten', 'worthi', 'watch'): 1,\n",
              "          ('worthi', 'watch', 'ye'): 1,\n",
              "          ('watch', 'ye', 'amaz'): 1,\n",
              "          ('ye', 'amaz', 'went'): 1,\n",
              "          ('amaz', 'went', 'film'): 1,\n",
              "          ('went', 'film', 'expect'): 1,\n",
              "          ('film', 'expect', 'allie'): 1,\n",
              "          ('expect', 'allie', 'classic'): 1,\n",
              "          ('allie', 'classic', 'base'): 1,\n",
              "          ('classic', 'base', 'review'): 1,\n",
              "          ('base', 'review', 'read'): 1,\n",
              "          ('review', 'read', 'end'): 1,\n",
              "          ('read', 'end', 'watch'): 1,\n",
              "          ('end', 'watch', 'decent'): 1,\n",
              "          ('watch', 'decent', 'film'): 1,\n",
              "          ('decent', 'film', 'overal'): 1,\n",
              "          ('film', 'overal', 'phoenix'): 1,\n",
              "          ('overal', 'phoenix', 'outstand'): 1,\n",
              "          ('phoenix', 'outstand', 'definit'): 1,\n",
              "          ('outstand', 'definit', 'part'): 1,\n",
              "          ('definit', 'part', 'feel'): 1,\n",
              "          ('part', 'feel', 'like'): 1,\n",
              "          ('feel', 'like', 'you'): 1,\n",
              "          ('like', 'you', 'seen'): 1,\n",
              "          ('you', 'seen', 'trailer'): 1,\n",
              "          ('seen', 'trailer', 'you'): 1,\n",
              "          ('trailer', 'you', 'seen'): 1,\n",
              "          ('you', 'seen', 'movi'): 1,\n",
              "          ('seen', 'movi', 'best'): 1,\n",
              "          ('movi', 'best', 'way'): 1,\n",
              "          ('best', 'way', 'describ'): 1,\n",
              "          ('way', 'describ', 'film'): 1,\n",
              "          ('describ', 'film', 'feel'): 1,\n",
              "          ('film', 'feel', 'like'): 1,\n",
              "          ('feel', 'like', '2'): 1,\n",
              "          ('like', '2', 'hour'): 1,\n",
              "          ('2', 'hour', 'version'): 1,\n",
              "          ('hour', 'version', 'trailer'): 1,\n",
              "          ('version', 'trailer', '2'): 1,\n",
              "          ('trailer', '2', 'hour'): 1,\n",
              "          ('2', 'hour', 'rel'): 1,\n",
              "          ('hour', 'rel', 'common'): 1,\n",
              "          ('rel', 'common', 'aunti'): 1,\n",
              "          ('common', 'aunti', 'felt'): 1,\n",
              "          ('aunti', 'felt', 'lot'): 1,\n",
              "          ('felt', 'lot', 'longerwhich'): 1,\n",
              "          ...})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHS1T2dvxvqH"
      },
      "source": [
        "from collections import Counter\r\n",
        "BG = nltk.bigrams(Repeat)\r\n",
        "FD = nltk.FreqDist(BG)\r\n",
        "BG_dict = dict(FD)\r\n",
        "for i in BG_dict:\r\n",
        "  print( str(i) + ':' + str(BG_dict[i] / Repeat.count(i[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "bPtmgFi2yR36",
        "outputId": "76c4de9c-79e1-4cdc-fce6-756e8270dc1b"
      },
      "source": [
        "complete_data = ''\r\n",
        "j= []\r\n",
        "count = 1\r\n",
        "for i in Data_Frame['After Lemmatization']:\r\n",
        "  complete_data = complete_data + i\r\n",
        "  k = 'Review-' + str(i)\r\n",
        "  j.append(k)\r\n",
        "  count+= 1\r\n",
        "\r\n",
        "from textblob import TextBlob\r\n",
        "nph = []\r\n",
        "fcy = []\r\n",
        "for i in Data_Frame['After Lemmatization']:\r\n",
        "  blob = TextBlob(i)\r\n",
        "  for nouns in blob.noun_phrases:\r\n",
        "    np.append(nouns)\r\n",
        "for w in np:\r\n",
        "  npf = []\r\n",
        "  for i in Data_Frame['After Lemmatization']:\r\n",
        "    npf.append(i.count(w) / complete_data.count(w))\r\n",
        "  fcy.append(np)\r\n",
        "noun_phrases_Data = pd.DataFrame(fcy).T\r\n",
        "noun_phrases_Data.columns = list(np)\r\n",
        "noun_phrases_Data.j = j\r\n",
        "noun_phrases_Data"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-28150722a10f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mnouns\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoun_phrases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mnpf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: _append_dispatcher() missing 1 required positional argument: 'values'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "a14080b2-d653-4a6c-fd69-623d0dde15fb"
      },
      "source": [
        "# Write your code here\n",
        "import math\n",
        "def value_tf(s,w):\n",
        "  length = len(s.split(\" \"))\n",
        "  value_of_TF = s.count(w)/length\n",
        "  value_of_IDF = 0\n",
        "  if(value_of_TF!=0):\n",
        "    value_of_IDF = math.log(length)/s.count(w)\n",
        "  else:\n",
        "    return 0;\n",
        "  return value_of_TF*value_of_IDF\n",
        "\n",
        "sentence = Data_Frame[\"After Spelling Correction\"].values.tolist()\n",
        "tl = set([j for i in sent for j in i.split(\" \")])\n",
        "IDF_TF = pd.DataFrame(tl,columns=[\"token\"])\n",
        "value = 0\n",
        "for i in sentence:\n",
        "  IDF_TF[str(count)] = IDF_TF[\"token\"].apply(lambda x : value_tf(i,x))\n",
        "  value=value+1\n",
        "\n",
        "IDF_TF\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>0.21799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>boring</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>expel</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>deserving</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>trollworthy</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2275</th>\n",
              "      <td>notably</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2276</th>\n",
              "      <td>reviews</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2277</th>\n",
              "      <td>finally</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2278</th>\n",
              "      <td>reaction</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2279</th>\n",
              "      <td>especially</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2280 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            token      126\n",
              "0                  0.21799\n",
              "1          boring  0.00000\n",
              "2           expel  0.00000\n",
              "3       deserving  0.00000\n",
              "4     trollworthy  0.00000\n",
              "...           ...      ...\n",
              "2275      notably  0.00000\n",
              "2276      reviews  0.00000\n",
              "2277      finally  0.00000\n",
              "2278     reaction  0.00000\n",
              "2279   especially  0.00000\n",
              "\n",
              "[2280 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "qJaVU8nz7awt",
        "outputId": "2018220a-52fd-495a-b093-00852c6d50a5"
      },
      "source": [
        "from nltk.corpus import stopwords \r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "search_sentence=\"This is a movie that only those who have felt alone and isolated can truly relate to it. You understand the motive and you feel sorry for the character. A lot of people will see this movie and think that it encourages violence. But truly, this movie should encourage each and every one of us to become a better person, treat everyone with respect and make each other feel like they belong in this world, instead of making them feel isolated. .\"\r\n",
        "\r\n",
        "X_list = word_tokenize(search_sentence)\r\n",
        "stop_word = stopwords.words('english') \r\n",
        "X_set = {i for i in X_list if not i in stop_word}\r\n",
        "\r\n",
        "def cv(X_set, Y_set):\r\n",
        "  r_vector = X_set.union(Y_set)\r\n",
        "  list1 =[];list2 =[]\r\n",
        "  for j in r_vector: \r\n",
        "      if j in X_set: list1.append(1)\r\n",
        "      else: list1.append(0) \r\n",
        "      if j in Y_set: list2.append(1) \r\n",
        "      else: list2.append(0)\r\n",
        "  return r_vector, list1, list2\r\n",
        "\r\n",
        "def cc(r_vector, l1, l2):\r\n",
        "  count = 0\r\n",
        "  for i in range(len(r_vector)): \r\n",
        "        count+= list1[i]*list2[i] \r\n",
        "  c = count / float((sum(list1)*sum(list2))**0.5) \r\n",
        "  return c\r\n",
        "\r\n",
        "\r\n",
        "list3 = []\r\n",
        "for l in Data_Frame['After Spelling Correction']:\r\n",
        "  Y_list = word_tokenize(l)\r\n",
        "  Y_set = {k for k in Y_list if not k in stop_word}\r\n",
        "  r_vector, list1, list2 = cv(X_set, Y_set)\r\n",
        "  try:\r\n",
        "    similarity = cc(r_vector, list1, list2)\r\n",
        "  except ZeroDivisionError:\r\n",
        "    similarity = 'None'\r\n",
        "  list3.append(similarity)\r\n",
        "Data_2 = pd.DataFrame(list(zip(Data_Frame['Reviews'],list3)), columns=['Reviews','cosine cimilarity'])\r\n",
        "Data_2"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reviews</th>\n",
              "      <th>cosine cimilarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "      <td>0.097503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
              "      <td>0.0723102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Joaquin Phoenix gives a tour de force performa...</td>\n",
              "      <td>0.0432136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "      <td>0.0218687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>I didnt really know much about this movie. I h...</td>\n",
              "      <td>0.115517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>Film was directed and produced poorly. Opportu...</td>\n",
              "      <td>0.0672673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>Wow, what a movie! I have to admit, When I fir...</td>\n",
              "      <td>0.0792118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>This movie is poorly done as to how it tells t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Reviews cosine cimilarity\n",
              "0    Every once in a while a movie comes, that trul...                 0\n",
              "1    This is a movie that only those who have felt ...          0.097503\n",
              "2    Truly a masterpiece, The Best Hollywood film o...         0.0723102\n",
              "3    Joaquin Phoenix gives a tour de force performa...         0.0432136\n",
              "4    Most of the time movies are anticipated like t...         0.0218687\n",
              "..                                                 ...               ...\n",
              "120  I didnt really know much about this movie. I h...          0.115517\n",
              "121                                                                 None\n",
              "122  Film was directed and produced poorly. Opportu...         0.0672673\n",
              "123  Wow, what a movie! I have to admit, When I fir...         0.0792118\n",
              "124  This movie is poorly done as to how it tells t...                 0\n",
              "\n",
              "[125 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "# Link: https://github.com/mushamvineeth/Vineeth_INFO5731_Spring2021/blob/main/Reviews.numbers\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}